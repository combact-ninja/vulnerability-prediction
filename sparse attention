import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from keras.utils import to_categorical

# Create random features and labels
X = np.random.rand(1000, 17, 1)
y = np.random.randint(0, 10, 1000)  # Assuming 10 classes for classification

# Normalize the data
scaler = StandardScaler()
X = X.reshape(-1, 17)  # Flatten for scaling
X = scaler.fit_transform(X)
X = X.reshape(1000, 17, 1)  # Reshape back to original shape

# Categorize the labels
y = to_categorical(y, num_classes=10)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import tensorflow as tf
from tensorflow.keras.layers import Layer
from tensorflow.keras import backend as K

class SparseSelfAttention(Layer):
    def __init__(self, num_heads=4, sparsity_config=None, **kwargs):
        super(SparseSelfAttention, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.sparsity_config = sparsity_config

    def build(self, input_shape):
        self.q_dense = tf.keras.layers.Dense(input_shape[-1])
        self.k_dense = tf.keras.layers.Dense(input_shape[-1])
        self.v_dense = tf.keras.layers.Dense(input_shape[-1])
        self.output_dense = tf.keras.layers.Dense(input_shape[-1])

    def call(self, inputs):
        query, key, value = inputs, inputs, inputs
        batch_size = tf.shape(query)[0]

        # Linear projections
        query = self.q_dense(query)
        key = self.k_dense(key)
        value = self.v_dense(value)

        # Split and concatenate for multi-head attention
        def split_heads(x, batch_size):
            x = tf.reshape(x, (batch_size, -1, self.num_heads, x.shape[-1] // self.num_heads))
            return tf.transpose(x, perm=[0, 2, 1, 3])

        query = split_heads(query, batch_size)
        key = split_heads(key, batch_size)
        value = split_heads(value, batch_size)

        # Scaled dot-product attention
        matmul_qk = tf.matmul(query, key, transpose_b=True)
        dk = tf.cast(tf.shape(key)[-1], tf.float32)
        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)

        # Add sparse attention logic here if needed
        # ...

        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)
        output = tf.matmul(attention_weights, value)

        # Concatenate heads and put through final linear layer
        output = tf.transpose(output, perm=[0, 2, 1, 3])
        concat_output = tf.reshape(output, (batch_size, -1, output.shape[-1] * self.num_heads))

        output = self.output_dense(concat_output)
        return output

    def compute_output_shape(self, input_shape):
        return input_shape

    def get_config(self):
        config = super(SparseSelfAttention, self).get_config()
        config.update({
            "num_heads": self.num_heads,
            "sparsity_config": self.sparsity_config,
        })
        return config



from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense

input_layer = Input(shape=(17, 1))
lstm_layer = LSTM(64, return_sequences=True)(input_layer)
attention_layer = SparseSelfAttention(num_heads=4)(lstm_layer)
lstm_layer_2 = LSTM(64)(attention_layer)
output_layer = Dense(10, activation='softmax')(lstm_layer_2)

model = Model(inputs=input_layer, outputs=output_layer)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()


model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc}")
