###################### from pretrained word2vec model ####################################

import requests

url = "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
response = requests.get(url, stream=True)

with open("GoogleNews-vectors-negative300.bin.gz", "wb") as f:
    for chunk in response.iter_content(chunk_size=1024):
        if chunk:
            f.write(chunk)

print("Download complete!")


# Load Google's pre-trained Word2Vec model
model_path = 'GoogleNews-vectors-negative300.bin.gz'
model = KeyedVectors.load_word2vec_format(model_path, binary=True)

# Get the vector for a single word
word_vector = model['example']
print("Vector for 'example':")
print(word_vector)

def get_sentence_vector(sentence, model):
    # Remove words not in the vocabulary
    words = [word for word in sentence if word in model.key_to_index]
    if not words:
        return np.zeros(model.vector_size)
    
    # Get vectors for each word and calculate the mean
    word_vectors = [model[word] for word in words]
    return np.mean(word_vectors, axis=0)

# Example sentence
sentence = ['this', 'is', 'a', 'sample', 'sentence']
sentence_vector = get_sentence_vector(sentence, model)
print("\nVector for the sentence 'this is a sample sentence':")
print(sentence_vector)

##########################  word2vec ######################################
import gensim
from gensim.models import Word2Vec

# Sample corpus
sentences = [
    ['this', 'is', 'a', 'sample', 'sentence'],
    ['word2vec', 'is', 'a', 'technique', 'for', 'natural', 'language', 'processing'],
    ['another', 'example', 'sentence'],
]

# Train the Word2Vec model
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

import numpy as np

def get_sentence_vector(sentence, model):
    # Remove words not in the vocabulary
    words = [word for word in sentence if word in model.wv.key_to_index]
    if not words:
        return np.zeros(model.vector_size)
    
    # Get vectors for each word and calculate the mean
    word_vectors = [model.wv[word] for word in words]
    return np.mean(word_vectors, axis=0)

# Example sentence
sentence = ['e445r', 'sdsa', 'ass', 'elif', 'sentence']
sentence_vector = get_sentence_vector(sentence, model)
print(sentence_vector)
############################################ Autoencoder Feature ##############################




# Convert sentences to vectors
sentence_vectors = np.array([get_sentence_vector(sentence, model) for sentence in sentences])
print("Sentence Vectors:\n", sentence_vectors)

# Define the dimensions of the input and the encoded representation
input_dim = sentence_vectors.shape[1]
encoding_dim = 50  # You can change this to the desired dimensionality

# Input placeholder
input_sentence = Input(shape=(input_dim,))

# Encoded representation
encoded = Dense(encoding_dim, activation='relu')(input_sentence)

# Reconstruction of the input
decoded = Dense(input_dim, activation='sigmoid')(encoded)

# Autoencoder model
autoencoder = Model(input_sentence, decoded)

# Encoder model to get the encoded features
encoder = Model(input_sentence, encoded)

# Compile the autoencoder
autoencoder.compile(optimizer='adam', loss='mse')

# Train the autoencoder
autoencoder.fit(sentence_vectors, sentence_vectors, epochs=100, batch_size=4, shuffle=True)

# Get the encoded features
encoded_features = encoder.predict(sentence_vectors)
print("Encoded Features:\n", encoded_features)
