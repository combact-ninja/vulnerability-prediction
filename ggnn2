import tensorflow as tf
from tensorflow.keras.layers import Layer, Dense
from tensorflow.keras import Model

class GatedGraphUnit(Layer):
    def __init__(self, units):
        super(GatedGraphUnit, self).__init__()
        self.units = units
        self.dense_z = Dense(units, activation='sigmoid')
        self.dense_r = Dense(units, activation='sigmoid')
        self.dense_h = Dense(units, activation='tanh')

    def call(self, x, adj):
        # x: Node features [batch_size, num_nodes, feature_dim]
        # adj: Adjacency matrix [batch_size, num_nodes, num_nodes]
        
        # Update gate
        z = self.dense_z(x)
        
        # Reset gate
        r = self.dense_r(x)
        
        # Candidate hidden state
        h_tilde = self.dense_h(tf.matmul(adj, r * x))
        
        # New hidden state
        h = (1 - z) * x + z * h_tilde
        
        return h

class GatedGraphSequenceNN(Model):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        super(GatedGraphSequenceNN, self).__init__()
        self.num_layers = num_layers
        self.ggus = [GatedGraphUnit(hidden_dim) for _ in range(num_layers)]
        self.dense_in = Dense(hidden_dim)
        self.dense_out = Dense(output_dim)

    def call(self, x, adj):
        x = self.dense_in(x)  # Project input to hidden_dim
        for i in range(self.num_layers):
            x = self.ggus[i](x, adj)
        x = tf.reshape(x, (x.shape[0], -1))
        output = self.dense_out(x)
        return output

# Sample data for demonstration
def generate_sample_data(num_samples, num_nodes, feature_dim):
    node_features = tf.random.uniform((num_samples, num_nodes, feature_dim))
    adj_matrix = tf.random.uniform((num_samples, num_nodes, num_nodes))
    labels = tf.random.uniform((num_samples, 2))
    return node_features, adj_matrix, labels

# Loss function
loss_object = tf.keras.losses.MeanSquaredError()

# Optimizer
optimizer = tf.keras.optimizers.Adam()

# Training step
@tf.function
def train_step(model, node_features, adj_matrix, labels):
    with tf.GradientTape() as tape:
        predictions = model(node_features, adj_matrix)
        loss = loss_object(labels, predictions)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss

# Training function
def train(model, epochs, num_samples, num_nodes, feature_dim):
    for epoch in range(epochs):
        node_features, adj_matrix, labels = generate_sample_data(num_samples, num_nodes, feature_dim)
        loss = train_step(model, node_features, adj_matrix, labels)
        print(f'Epoch {epoch + 1}, Loss: {loss.numpy()}')

# Testing function
def test(model, num_samples, num_nodes, feature_dim):
    node_features, adj_matrix, labels = generate_sample_data(num_samples, num_nodes, feature_dim)
    predictions = model(node_features, adj_matrix)
    loss = loss_object(labels, predictions)
    print(f'Test Loss: {loss.numpy()}')

# Main
if __name__ == "__main__":
    # Hyperparameters
    num_samples = 1000
    num_nodes = 120
    feature_dim = 120
    hidden_dim = 64
    output_dim = 2
    num_layers = 2
    epochs = 10

    # Model
    model = GatedGraphSequenceNN(input_dim=feature_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)

    # Training
    train(model, epochs, num_samples, num_nodes, feature_dim)

    # Testing
    test(model, num_samples, num_nodes, feature_dim)
